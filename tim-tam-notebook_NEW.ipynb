{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9660011,"sourceType":"datasetVersion","datasetId":5901714},{"sourceId":9660018,"sourceType":"datasetVersion","datasetId":5901720}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing everything \nImportant: run this everytime!","metadata":{}},{"cell_type":"code","source":"#(ignore) Importing everything\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:17.763615Z","iopub.execute_input":"2024-10-22T16:51:17.764073Z","iopub.status.idle":"2024-10-22T16:51:17.770808Z","shell.execute_reply.started":"2024-10-22T16:51:17.764030Z","shell.execute_reply":"2024-10-22T16:51:17.769529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\nMencakup proses analisis untuk mendapatkan ide feature engineering, processing, dan insight yang didapatkan dari analisis data.","metadata":{}},{"cell_type":"markdown","source":"## Mari baca sekilas data yang kita punya!\nKita akan import datasets \"training\" untuk mendapat gambaran awal mengenai dataset kita","metadata":{}},{"cell_type":"code","source":"# Load train.csv\nkantor_file_path = pd.read_csv('../input/training/Train.csv')\n\n# Menampilkan data secara umum\nkantor_file_path.describe()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:17.772554Z","iopub.execute_input":"2024-10-22T16:51:17.772970Z","iopub.status.idle":"2024-10-22T16:51:17.905281Z","shell.execute_reply.started":"2024-10-22T16:51:17.772933Z","shell.execute_reply":"2024-10-22T16:51:17.904197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Menampilkan data bagian awal\nkantor_file_path.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:17.907170Z","iopub.execute_input":"2024-10-22T16:51:17.907504Z","iopub.status.idle":"2024-10-22T16:51:17.933137Z","shell.execute_reply.started":"2024-10-22T16:51:17.907470Z","shell.execute_reply":"2024-10-22T16:51:17.931936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dengan melihat dataset sekilas, kita dapat mengambil beberapa kesimpulan seperti:\n1. Beberapa row masih mengandung data kosong (NaN), hal ini akan mempengaruhi proses prediksi nanti. Baiknya kita mengisi data kosong ini dengan value yang tidak merubah keseluruhan data secara signifikan\n2. Beberapa data masih berupa text, kita harus mengubah text data ini menjadi suatu bilangan agar mudah dibaca oleh model kita.\n3. Terdapat banyak sekali features, kita akan berusaha untuk mencari mana features yang lebih berperan dalam proses prediksi.","metadata":{}},{"cell_type":"markdown","source":"# Metode\nMencakup langkah preprocessing dan feature engineering.","metadata":{}},{"cell_type":"markdown","source":"## Random Sampling untuk mengisi data kosong (NaN)\nDari proses EDA, kita mengerti bahwa terdapat beberapa data kosong yang akan mengganggu proses prediksi kita. Kali ini, kita akan menggunakan metode \"random sampling\" untuk mengisi data kosong tadi.\n\nMetode Random sampling untuk mengisi data kosong (NaN) adalah teknik di mana nilai-nilai yang hilang dalam dataset diisi menggunakan nilai-nilai yang diambil secara acak dari nilai yang sudah ada dalam kolom yang sama. Alhasil, dataset secara keseluruhan tidak akan berubah terlalu signifikan (maintain itself)","metadata":{}},{"cell_type":"code","source":"# Membuat salinan DataFrame untuk manipulasi\nkantor_data = kantor_file_path.copy()\n\n# Kolom yang akan diisi dengan random sampling\n# Kolom yang akan diisi dengan random sampling\ncolumns_to_impute = [\n    'Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EmployeeCount', \n    'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', \n    'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', \n    'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', \n    'StandardHours', 'StockOptionLevel', 'WorkLifeBalance', 'YearsWithCurrManager',\n    'BusinessTravel', 'Department', 'EducationField', 'Gender', \n    'OverTime', 'JobRole', 'MaritalStatus', 'OverTime'\n]\n\n\n# Mengisi nilai kosong dengan random sampling\nfor column in columns_to_impute:\n    # Mendapatkan semua nilai non-NaN dari kolom tersebut\n    non_nan_values = kantor_data[column].dropna().values  # Ubah ke array dengan .values\n    \n    # Fungsi untuk mengisi nilai kosong dengan random sampling\n    def impute_random_value():\n        return np.random.choice(non_nan_values)  # Mengambil sampel acak dari non-NaN\n    \n    # Terapkan imputasi random sampling ke kolom yang bersangkutan\n    kantor_data[column] = kantor_data[column].apply(lambda x: impute_random_value() if pd.isna(x) else x)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:17.934752Z","iopub.execute_input":"2024-10-22T16:51:17.935215Z","iopub.status.idle":"2024-10-22T16:51:18.399163Z","shell.execute_reply.started":"2024-10-22T16:51:17.935163Z","shell.execute_reply":"2024-10-22T16:51:18.397764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot Encoding untuk mengubah data kategorikal menjadi angka biner\nSelanjutnya, kita harus mengubah data kategorikal (seperti department, jobrole, dll) menjadi data angka yang bisa dibaca oleh model. Kita akan menggunakan one hot encoding untuk menyelesaikannya.\n\nOne hot encoding adalah metode yang digunakan untuk mengubah data kategorikal menjadi representasi biner agar dapat digunakan dalam algoritma pembelajaran mesin, yang umumnya hanya bekerja dengan data numerik. Proses ini mengonversi setiap kategori unik dalam variabel kategorikal menjadi kolom biner(0 atau 1).","metadata":{}},{"cell_type":"code","source":"# --- Kolom yang ingin digunakan ---\ncolumns_to_use = [\n  'Age', 'EnvironmentSatisfaction', 'NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', \n'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'OverTime', 'MaritalStatus'\n]\n\n# --- One-Hot Encoding untuk fitur kategorikal ---\nohe = OneHotEncoder(drop='first', sparse=False)\nX_encoded = pd.DataFrame(\n    ohe.fit_transform(\n        kantor_data[['BusinessTravel', 'Department', 'EducationField', 'Gender',\n                     'OverTime', 'JobRole', 'MaritalStatus']]\n    )\n)\n\n# Memberikan nama kolom baru setelah One-Hot Encoding\nX_encoded.columns = ohe.get_feature_names_out()\n\n# Gabungkan hasil OHE dengan data numerik lainnya\nkantor_data_final = pd.concat([kantor_data.drop(columns=['id','Attrition','EmployeeCount',\n                                                         'Over18','StandardHours','BusinessTravel', \n                                                         'Department', 'EducationField', 'Gender', \n                                                         'OverTime', 'JobRole', \n                                                         'MaritalStatus']), X_encoded], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.402249Z","iopub.execute_input":"2024-10-22T16:51:18.402753Z","iopub.status.idle":"2024-10-22T16:51:18.438917Z","shell.execute_reply.started":"2024-10-22T16:51:18.402681Z","shell.execute_reply":"2024-10-22T16:51:18.437719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Menampilkan beberapa baris hasil akhir\nkantor_data_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.440197Z","iopub.execute_input":"2024-10-22T16:51:18.440542Z","iopub.status.idle":"2024-10-22T16:51:18.472448Z","shell.execute_reply.started":"2024-10-22T16:51:18.440507Z","shell.execute_reply":"2024-10-22T16:51:18.470983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dengan begini, kita berhasil mengisi semua data kosong dan mengubah semua data kategorikal menjadi angka biner.","metadata":{}},{"cell_type":"markdown","source":"## Menentukan features yang akan dipakai dan target prediksi\nUntuk sementara, kita akan menggunakan seluruh features yang ada. Disamping itu, kita tahu bahwa variabel yang kita cari adalah \"Attritions\"","metadata":{}},{"cell_type":"code","source":"#assign features (variabel kolom) yang akan kita pakai dalam algoritma prediksi\nkantor_features = ['Age', 'DailyRate', 'DistanceFromHome', 'Education',\n       'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel',\n       'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager',\n       'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely',\n       'Department_Research & Development', 'Department_Sales',\n       'EducationField_Life Sciences', 'EducationField_Marketing',\n       'EducationField_Medical', 'EducationField_Other',\n       'EducationField_Technical Degree', 'Gender_Male', 'OverTime_Yes',\n       'JobRole_Human Resources', 'JobRole_Laboratory Technician',\n       'JobRole_Manager', 'JobRole_Manufacturing Director',\n       'JobRole_Research Director', 'JobRole_Research Scientist',\n       'JobRole_Sales Executive', 'JobRole_Sales Representative',\n       'MaritalStatus_Married', 'MaritalStatus_Single']","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.473745Z","iopub.execute_input":"2024-10-22T16:51:18.474077Z","iopub.status.idle":"2024-10-22T16:51:18.480939Z","shell.execute_reply.started":"2024-10-22T16:51:18.474042Z","shell.execute_reply":"2024-10-22T16:51:18.479690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign X sebagai variabel (features) yg digunakan untuk memprediksi\nX = kantor_data_final[kantor_features]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.482522Z","iopub.execute_input":"2024-10-22T16:51:18.483109Z","iopub.status.idle":"2024-10-22T16:51:18.497543Z","shell.execute_reply.started":"2024-10-22T16:51:18.482955Z","shell.execute_reply":"2024-10-22T16:51:18.496384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign y sebagai variabel yg ingin diprediksi (atrisi)\ny = kantor_data.Attrition","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.498827Z","iopub.execute_input":"2024-10-22T16:51:18.499216Z","iopub.status.idle":"2024-10-22T16:51:18.507358Z","shell.execute_reply.started":"2024-10-22T16:51:18.499177Z","shell.execute_reply":"2024-10-22T16:51:18.506256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluasi**\nMencakup modelling, evaluasi, dan analisis hasil prediksi.","metadata":{}},{"cell_type":"markdown","source":"## Modeling dengan RandomForestClassifier\nSekarang kita akan mulai tahap modeling, tepatnya kita akan menggunakan estimator \"RandomForestClassifier\" karena data prediksi yang diinginkan adalah value \"true\" atau \"false\".\n\nRandom Forest Classifier adalah algoritma pembelajaran mesin berbasis ensemble yang digunakan untuk tugas klasifikasi. Algoritma ini bekerja dengan membangun banyak decision trees selama pelatihan, dan kemudian menggabungkan hasilnya (melalui voting untuk klasifikasi) untuk menghasilkan prediksi yang lebih akurat dan stabil.\n\n","metadata":{}},{"cell_type":"code","source":"#ini kalo treeclassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#setup random seed. Why? idk. Biar sama trs kali?\nnp.random.seed(42)\n\n#define \"Kantor_model\" as model (algoritma) yang kita pake\n#Specify a number for random_state to ensure same results each run\nkantor_model = RandomForestClassifier(n_estimators=100)\n\n#melatih model dengan X, lalu si mesin bakal nyari pola agar hasilnya jadi y\nkantor_model.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:18.508505Z","iopub.execute_input":"2024-10-22T16:51:18.508872Z","iopub.status.idle":"2024-10-22T16:51:19.932219Z","shell.execute_reply.started":"2024-10-22T16:51:18.508812Z","shell.execute_reply":"2024-10-22T16:51:19.930952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ngecek f1 score dr model yg udah kita latih¶\n\"predicted_attritions\" kita isi dengan hasil prediksi \"kantor_model\" dengan input variabel (features) X. Lalu aku mencari f1 scores dengan membandingkan y (hasil sebenarnya) dengan \"predicted_attritions (hasil prediksiku)\n\nIdeally ini nilainya 1.0","metadata":{}},{"cell_type":"code","source":"#ngecek f1 score dr model yg udah kita latih\nfrom sklearn.metrics import f1_score\npredicted_attritions = kantor_model.predict(X)\nf1_score(y, predicted_attritions)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:19.936964Z","iopub.execute_input":"2024-10-22T16:51:19.937372Z","iopub.status.idle":"2024-10-22T16:51:20.083444Z","shell.execute_reply.started":"2024-10-22T16:51:19.937332Z","shell.execute_reply":"2024-10-22T16:51:20.082326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Nilai perfect doesn't mean modelnya perfect¶\nalgoritma model yang kita pilih adalah \"RandomForestClassifier\" dgn n_estimators=100. Idealnya, algoritma model yg sempurna adalah model yang kalau kita ngetrain pake dataset apapun HARUSNYA hasil prediksinya bener terus. Nah, kita mau ngecek nih kalo model kita cmn dilatih dengan setengah dr dataset awal kira-kira bisa menghasilkan prediksi yang bener engga.\n\nKita pake \"train_test_split\" untuk membelah data train jadi 2 bagian, anggep aja kalo datanya ada 100 aku bagi jadi 50 pasang x dan y.\n\nyg satu namanya train_x (variabel latih) dan train_y (prediksi yang diinginkan dr latihan)\nyg satu lagi namanya val_x (variabel validasi) dan val_y (prediksi validasi)\nintinya, kita buang model sebelumnya, sekarang kita latih modelnya pake dataset baru (melatih model dengan train_X, lalu si mesin akan ,mencari pola agar hasilnya jadi train_y)\n\nnah setelah polanya tersimpan, sekarang kita suruh prediksi dengan input \"val_X\", nanti hasil prediksinya kita masukin ke \"val_predictions\"\n\n\"val_predictions\" ini kita bandingkan dengan \"val_y\" untuk mengecek f1 scorenya. KALAU polanya sempurna, harusnya f1 scorenya 1.0 karena ideally hasil prediksi kita sama seperti \"val_y\"","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# ini ngesplit datanya, dikasih randomstate biar kebelahnya di posisi yang sama terus\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# define \"Kantor_model\" as model (algoritma) yang kita pake\nkantor_model = RandomForestClassifier(max_depth=50, max_features=None, n_estimators=100, random_state=1)\n\n# melatih model dengan train_X, lalu si mesin bakal nyari pola agar hasilnya jadi train_y\nkantor_model.fit(train_X, train_y)\n\n#check akurat\nprint(kantor_model.score(val_X, val_y))\n\n# ngecek f1 score dengan mbandingin val_y (prediksi valid)\n# dgn val_predictions (hasil prediksi dgn input val_X)\nval_predictions = kantor_model.predict(val_X)\nprint(f1_score(val_y, val_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:20.084789Z","iopub.execute_input":"2024-10-22T16:51:20.085228Z","iopub.status.idle":"2024-10-22T16:51:25.073068Z","shell.execute_reply.started":"2024-10-22T16:51:20.085189Z","shell.execute_reply":"2024-10-22T16:51:25.072025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meningkatkan akurasi dan presisi model kita\nDari sini, kita paham bahwa model kita tidak sempurna. Random Forest *by itself* tidaklah cukup akurat untuk menghasilkan prediksi yang dapat diandalkan. Untuk meningkatkan tingkat akurasi model kita, ada beberapa hal yang dapat dilakukan:\n\n1. Mencari Feature yang penting\n2. Menggunakan model lain yang lebih cocok\n3. Tuning model","metadata":{}},{"cell_type":"markdown","source":"## Mencari Feature yang Penting\n*not every features are created equal*\n\nDalam melakukan prediksi, model akan menggunakan features sebagai pola dalam prosesnya. Namun, tidak semua features memiliki tingkat \"pengaruh\" yang sama. Kita akan mencari mana features yang paling berpengaruh dan mana yang kurang berpengaruh dengan metode SFS (Sequential Feature Selection)\n\nMetode Sequential Feature Selection (SFS) adalah salah satu teknik dalam feature selection yang digunakan untuk memilih subset fitur yang paling relevan dari data, dengan tujuan meningkatkan kinerja model dan mengurangi kompleksitas. SFS melibatkan pendekatan iteratif di mana fitur-fitur dipilih secara berurutan berdasarkan kontribusi mereka terhadap kinerja model. Terdapat dua metode SFS yaitu forward SFS (dimulai tanpa apapun, satu per satu menambah feature) dan backward SFS (dimulai dengan segalanya, satu per satu mengurangi feature)\n\nTepatnya, kita akan menggunakan metode Forward SFS.","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\n# Misalkan kita sudah memiliki data dan target\n# X -> data fitur\n# y -> target\n\n# Membagi data ke dalam training dan testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Pilih RandomForest sebagai estimator\nrf_model = RandomForestClassifier(random_state=42)\n\n# Buat Sequential Feature Selector (SFS) dengan mlxtend (forward selection)\nsfs = SFS(rf_model,\n          k_features=10,  # Memilih 10 fitur terbaik\n          forward=True,   # Forward selection\n          floating=False, # Floating bisa diaktifkan untuk fleksibilitas tambahan\n          scoring='f1',   # Gunakan F1 score untuk evaluasi\n          cv=5,           # 5-fold cross-validation untuk stabilitas\n          n_jobs=-1)      # Gunakan semua core CPU untuk efisiensi\n\n# Fit SFS pada data training\nsfs = sfs.fit(X_train, y_train)\n\n# Fitur terbaik yang dipilih\nselected_features = list(sfs.k_feature_names_)\n\n# Transformasi data dengan fitur yang dipilih\nX_train_selected = sfs.transform(X_train)\nX_test_selected = sfs.transform(X_test)\n\n# Latih ulang model dengan fitur yang dipilih\nrf_model.fit(X_train_selected, y_train)\n\n# Prediksi dan evaluasi model\ny_pred = rf_model.predict(X_test_selected)\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1}\")\n\n# Menampilkan fitur yang dipilih\nprint(f\"Selected Features: {selected_features}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:51:25.074571Z","iopub.execute_input":"2024-10-22T16:51:25.075053Z","iopub.status.idle":"2024-10-22T16:57:10.189787Z","shell.execute_reply.started":"2024-10-22T16:51:25.075004Z","shell.execute_reply":"2024-10-22T16:57:10.188397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nah, sekarang kita akan assign ulang features yang digunakan dengan hasil dari metode Forward SFS tadi.","metadata":{}},{"cell_type":"code","source":"# Load file CSV\nkantor_file_path = pd.read_csv('../input/training/Train.csv')\n\n# Membuat salinan DataFrame untuk manipulasi\nkantor_data = kantor_file_path.copy()\n\n# Kolom yang akan diisi dengan random sampling dari fitur yang dipilih oleh SFS\ncolumns_to_impute = ['Age','NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n    'YearsWithCurrManager', 'Department', 'OverTime', 'MaritalStatus']\n# Mengisi nilai kosong dengan random sampling\nfor column in columns_to_impute:\n    # Mendapatkan semua nilai non-NaN dari kolom tersebut\n    non_nan_values = kantor_data[column].dropna().values  # Ubah ke array dengan .values\n    \n    # Fungsi untuk mengisi nilai kosong dengan random sampling\n    def impute_random_value():\n        return np.random.choice(non_nan_values)  # Mengambil sampel acak dari non-NaN\n    \n    # Terapkan imputasi random sampling ke kolom yang bersangkutan\n    kantor_data[column] = kantor_data[column].apply(lambda x: impute_random_value() if pd.isna(x) else x)\n\n# --- Kolom yang ingin digunakan ---\ncolumns_to_use = ['Age','NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n    'YearsWithCurrManager', 'Department', 'OverTime', 'MaritalStatus']\n\n# --- One-Hot Encoding untuk fitur kategorikal ---\ncategorical_columns = ['Department', 'OverTime','MaritalStatus']\n\n# Menggunakan One-Hot Encoder hanya pada kolom kategorikal\nohe = OneHotEncoder(drop='first', sparse=False)\nX_encoded = pd.DataFrame(\n    ohe.fit_transform(kantor_data[categorical_columns])\n)\n\n# Memberikan nama kolom baru setelah One-Hot Encoding\nX_encoded.columns = ohe.get_feature_names_out(categorical_columns)\n\n# Gabungkan hasil OHE dengan data numerik lainnya menjadi kesatuan data yang digunakan untuk pelatihan\nkantor_data_final = pd.concat([kantor_data[columns_to_use].drop(columns=categorical_columns), X_encoded], axis=1)\n\nkantor_data_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:10.191603Z","iopub.execute_input":"2024-10-22T16:57:10.192023Z","iopub.status.idle":"2024-10-22T16:57:10.382519Z","shell.execute_reply.started":"2024-10-22T16:57:10.191973Z","shell.execute_reply":"2024-10-22T16:57:10.381281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign features (variabel kolom) yang akan kita pakai dalam algoritma prediksi setelah SFS\nkantor_features = ['Age', 'NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n                   'YearsWithCurrManager', 'Department_Research & Development', 'OverTime_Yes', 'MaritalStatus_Single']\n\n#assign X sebagai variabel (features) yg digunakan untuk memprediksi\nX = kantor_data_final[kantor_features]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:10.384219Z","iopub.execute_input":"2024-10-22T16:57:10.384699Z","iopub.status.idle":"2024-10-22T16:57:10.392370Z","shell.execute_reply.started":"2024-10-22T16:57:10.384647Z","shell.execute_reply":"2024-10-22T16:57:10.391034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Menggunakan atau menambah model yang digunakan\nUntuk kasus kali ini, kita akan menggunakan lebih dari satu model dengan metode stacking.\n\nStacking (atau stacked generalization) adalah metode ensemble learning di mana beberapa model pembelajaran mesin (disebut base models atau level-1 models) digabungkan untuk meningkatkan kinerja prediksi.\n\nKita akan menggunakan estimator RandomForestClassifier dan GradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"# Membagi data ke dalam training dan testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Model dasar (base models)\n# Menggunakan RandomForest dan Support Vector Machine sebagai base models\nbase_models = [\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('gb', GradientBoostingClassifier(random_state=42))  # SVC dengan probability=True untuk stacking\n]\n\n# Model meta (final estimator)\n# Menggunakan Logistic Regression sebagai meta-model\nmeta_model = LogisticRegression()\n\n# Stacking Classifier\nstacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n\n# Fit Stacking Classifier pada data training\nstacking_clf.fit(X_train, y_train)\n\n# Prediksi pada data testing\ny_pred = stacking_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:10.394041Z","iopub.execute_input":"2024-10-22T16:57:10.394448Z","iopub.status.idle":"2024-10-22T16:57:16.906404Z","shell.execute_reply.started":"2024-10-22T16:57:10.394412Z","shell.execute_reply":"2024-10-22T16:57:16.905185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluasi model dengan F1 Score\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score Stacking Classifier: {f1}\")\n\n# Menampilkan hasil prediksi\nprint(f\"Prediksi: {y_pred}\")\nval_predictions = stacking_clf.predict(X_test)\nprint(f1_score(y_test, val_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:16.908235Z","iopub.execute_input":"2024-10-22T16:57:16.908789Z","iopub.status.idle":"2024-10-22T16:57:16.975159Z","shell.execute_reply.started":"2024-10-22T16:57:16.908732Z","shell.execute_reply":"2024-10-22T16:57:16.973951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model akhir\nKita akan stay dengan model ini. Saatnya retrain model kita","metadata":{}},{"cell_type":"code","source":"stacking_clf.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:16.976591Z","iopub.execute_input":"2024-10-22T16:57:16.976979Z","iopub.status.idle":"2024-10-22T16:57:25.558135Z","shell.execute_reply.started":"2024-10-22T16:57:16.976940Z","shell.execute_reply":"2024-10-22T16:57:25.556858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mencari Prediksi test.csv\nKita akan mencoba untuk prediksi dengan input test_data.","metadata":{}},{"cell_type":"markdown","source":"## Membaca file test.csv, random sampling, OHE, dan assign features.","metadata":{}},{"cell_type":"code","source":"# Load file CSV\nkantortest_file_path = pd.read_csv('../input/testing/Test.csv')\n\n# Membuat salinan DataFrame untuk manipulasi\nkantortest_data = kantortest_file_path.copy()\n\n# Kolom yang akan diisi dengan random sampling dari fitur yang dipilih oleh SFS\ncolumns_to_impute = ['Age','NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n    'YearsWithCurrManager', 'Department', 'OverTime', 'MaritalStatus']\n\n# Mengisi nilai kosong dengan random sampling\nfor column in columns_to_impute:\n    # Mendapatkan semua nilai non-NaN dari kolom tersebut\n    non_nan_values = kantortest_data[column].dropna().values  # Ubah ke array dengan .values\n    \n    # Fungsi untuk mengisi nilai kosong dengan random sampling\n    def impute_random_value():\n        return np.random.choice(non_nan_values)  # Mengambil sampel acak dari non-NaN\n    \n    # Terapkan imputasi random sampling ke kolom yang bersangkutan\n    kantortest_data[column] = kantortest_data[column].apply(lambda x: impute_random_value() if pd.isna(x) else x)\n\n#OHE PART    \n# --- Kolom yang ingin digunakan ---\ncolumns_to_use = ['Age','NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n    'YearsWithCurrManager', 'Department', 'OverTime', 'MaritalStatus']\n\n# --- One-Hot Encoding untuk fitur kategorikal ---\ncategorical_columns = ['Department', 'OverTime','MaritalStatus']\n\n# Menggunakan One-Hot Encoder hanya pada kolom kategorikal\nohe = OneHotEncoder(drop='first', sparse=False)\nX_encoded = pd.DataFrame(\n    ohe.fit_transform(kantortest_data[categorical_columns])\n)\n\n# Memberikan nama kolom baru setelah One-Hot Encoding\nX_encoded.columns = ohe.get_feature_names_out(categorical_columns)\n\n# Gabungkan hasil OHE dengan data numerik lainnya menjadi kesatuan data yang digunakan untuk pelatihan\nkantortest_data_final = pd.concat([kantortest_data[columns_to_use].drop(columns=categorical_columns), X_encoded], axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.560033Z","iopub.execute_input":"2024-10-22T16:57:25.561307Z","iopub.status.idle":"2024-10-22T16:57:25.756826Z","shell.execute_reply.started":"2024-10-22T16:57:25.561243Z","shell.execute_reply":"2024-10-22T16:57:25.755769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Menampilkan beberapa baris hasil akhir\nkantortest_data_final.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.758729Z","iopub.execute_input":"2024-10-22T16:57:25.759085Z","iopub.status.idle":"2024-10-22T16:57:25.778412Z","shell.execute_reply.started":"2024-10-22T16:57:25.759047Z","shell.execute_reply":"2024-10-22T16:57:25.777201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign features (variabel kolom) yang akan kita pakai dalam algoritma prediksi\nkantortest_features = ['Age', 'NumCompaniesWorked', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsInCurrentRole', 'YearsSinceLastPromotion', \n                   'YearsWithCurrManager', 'Department_Research & Development', 'OverTime_Yes', 'MaritalStatus_Single']","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.779843Z","iopub.execute_input":"2024-10-22T16:57:25.780181Z","iopub.status.idle":"2024-10-22T16:57:25.791191Z","shell.execute_reply.started":"2024-10-22T16:57:25.780146Z","shell.execute_reply":"2024-10-22T16:57:25.789768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nge-assign variabel (features) test_data yg akan kita pake\nX_test = kantortest_data_final[kantortest_features]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.792910Z","iopub.execute_input":"2024-10-22T16:57:25.793451Z","iopub.status.idle":"2024-10-22T16:57:25.809730Z","shell.execute_reply.started":"2024-10-22T16:57:25.793394Z","shell.execute_reply":"2024-10-22T16:57:25.808419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediksi","metadata":{}},{"cell_type":"code","source":"#memprediksi pake input \"X_test\" dengan model yg udah kita batesin jumlah cabangnya\npredictions = stacking_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.811307Z","iopub.execute_input":"2024-10-22T16:57:25.811680Z","iopub.status.idle":"2024-10-22T16:57:25.959468Z","shell.execute_reply.started":"2024-10-22T16:57:25.811642Z","shell.execute_reply":"2024-10-22T16:57:25.958165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Membuat file csv untuk di-submit","metadata":{}},{"cell_type":"code","source":"#ini cuman buat nambahin kolom \"Attrition\" di file output csv-nya\nkantortest_data['Attrition'] = predictions","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.961326Z","iopub.execute_input":"2024-10-22T16:57:25.962245Z","iopub.status.idle":"2024-10-22T16:57:25.968983Z","shell.execute_reply.started":"2024-10-22T16:57:25.962184Z","shell.execute_reply":"2024-10-22T16:57:25.967735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Membuat DataFrame untuk hasil prediksi\nindex_values = range(len(predictions))  # Indeks dari 0 hingga panjang prediksi\nresults = pd.DataFrame({\n    'id': index_values,\n    'Attrition': ['TRUE' if pred >= 0.5 else 'FALSE' for pred in predictions]\n})\n\n# Menyimpan DataFrame ke file CSV\nresults.to_csv('predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:25.970783Z","iopub.execute_input":"2024-10-22T16:57:25.971385Z","iopub.status.idle":"2024-10-22T16:57:26.026846Z","shell.execute_reply.started":"2024-10-22T16:57:25.971333Z","shell.execute_reply":"2024-10-22T16:57:26.025528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Buat link untuk mendownload file\nFileLink('predictions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:57:26.028793Z","iopub.execute_input":"2024-10-22T16:57:26.029571Z","iopub.status.idle":"2024-10-22T16:57:26.037881Z","shell.execute_reply.started":"2024-10-22T16:57:26.029515Z","shell.execute_reply":"2024-10-22T16:57:26.036693Z"},"trusted":true},"execution_count":null,"outputs":[]}]}